{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lenovo\\\\Desktop\\\\Stage\\\\text-to-3D_Model_Generation\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lenovo\\\\Desktop\\\\Stage\\\\text-to-3D_Model_Generation'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class StyleGAN2Config:\n",
    "    outdir: Path\n",
    "    cfg: str\n",
    "    gpus: int\n",
    "    batch: int\n",
    "    gamma: float\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InferenceConfig:\n",
    "    ### Configs for inference\n",
    "    resume_pretrain: Path\n",
    "    inference_vis: bool\n",
    "    inference_to_generate_textured_mesh: bool\n",
    "    inference_save_interpolation: bool\n",
    "    inference_compute_fid: bool\n",
    "    inference_generate_geo: bool\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    ### Configs for dataset\n",
    "    data: Path\n",
    "    img_res: int\n",
    "    data_split_file: Path\n",
    "    use_labels: bool\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GeneratorConfig:\n",
    "    ### Configs for 3D generator\n",
    "    iso_surface: str\n",
    "    use_style_mixing: bool\n",
    "    one_3d_generator: bool\n",
    "    dmtet_scale: float\n",
    "    n_implicit_layer: int\n",
    "    feat_channel: int\n",
    "    mlp_latent_channel: int\n",
    "    deformation_multiplier: float\n",
    "    tri_plane_resolution: int\n",
    "    n_views: int\n",
    "    use_tri_plane: bool\n",
    "    tet_res: int\n",
    "    latent_dim: int\n",
    "    geometry_type: str\n",
    "    render_type: str\n",
    "@dataclass(frozen=True)\n",
    "class LossAndDiscriminatorConfig:\n",
    "    ### Configs for training loss and discriminator#\n",
    "    d_architecture: str\n",
    "    use_pl_length: bool\n",
    "    gamma_mask: float\n",
    "    d_reg_interval: int\n",
    "    add_camera_cond: bool\n",
    "    lambda_flexicubes_surface_reg: float\n",
    "    lambda_flexicubes_weights_reg: float\n",
    "@dataclass(frozen=True)\n",
    "class FeaturesConfig:\n",
    "    # Optional features.\n",
    "    cond: bool\n",
    "    freezed: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HyperparametersConfig:\n",
    "    # hyperparameters:\n",
    "    batch_gpu: int\n",
    "    cbase: int\n",
    "    cmax: int\n",
    "    glr: float\n",
    "    dlr: float\n",
    "    map_depth: int\n",
    "    mbstd_group: int\n",
    "@dataclass(frozen=True)\n",
    "class SettingsConfig:\n",
    "    # settings:\n",
    "    desc: str\n",
    "    metrics: list\n",
    "    kimg: int\n",
    "    tick: int\n",
    "    snap: int\n",
    "    seed: int\n",
    "    fp32: bool\n",
    "    nobench: bool\n",
    "    workers: int\n",
    "    dry_run: bool\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingModelConfig:\n",
    "    styleGAN2: StyleGAN2Config\n",
    "    inference: InferenceConfig\n",
    "    dataset: DatasetConfig\n",
    "    generator: GeneratorConfig\n",
    "    loss_and_discriminator: LossAndDiscriminatorConfig\n",
    "    features: FeaturesConfig\n",
    "    hyperparameters: HyperparametersConfig\n",
    "    settings: SettingsConfig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from textTo3DModelGen.constants import *\n",
    "from textTo3DModelGen.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self, \n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = HYPER_PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.params = self.params.model_hyperprams\n",
    "\n",
    "    def get_training_model_config(self) -> TrainingModelConfig:\n",
    "        config = self.config.training_prams\n",
    "        self.styleGAN2 = self.params.StyleGAN2\n",
    "        self.inference = self.params.inference\n",
    "        self.dataset = self.params.dataset\n",
    "        self.generator = self.params.generator\n",
    "        self.loss_and_discriminator = self.params.loss_and_discriminator\n",
    "        self.features = self.params.features\n",
    "        self.hyperparameters = self.params.hyperparameters\n",
    "        self.settings = self.params.settings\n",
    "\n",
    "        create_directories([\n",
    "            config.outdir  # , config.desc\n",
    "        ])\n",
    "\n",
    "        styleGAN2= StyleGAN2Config(\n",
    "            outdir = config.outdir,\n",
    "            cfg = self.styleGAN2.cfg,\n",
    "            gpus = config.gpus,\n",
    "            batch = config.batch,\n",
    "            gamma = config.gamma\n",
    "        )\n",
    "        inference= InferenceConfig(\n",
    "            resume_pretrain = self.inference.resume_pretrain,\n",
    "            inference_vis = self.inference.inference_vis,\n",
    "            inference_to_generate_textured_mesh = self.inference.inference_to_generate_textured_mesh,\n",
    "            inference_save_interpolation = self.inference.inference_save_interpolation,\n",
    "            inference_compute_fid = self.inference.inference_compute_fid,\n",
    "            inference_generate_geo = self.inference.inference_generate_geo\n",
    "        )\n",
    "        dataset= DatasetConfig(\n",
    "            data = config.data,\n",
    "            img_res = self.dataset.img_res,\n",
    "            data_split_file = config.data_split_file,\n",
    "            use_labels = self.dataset.use_labels\n",
    "        )\n",
    "        generator= GeneratorConfig(\n",
    "            iso_surface = self.generator.iso_surface,\n",
    "            use_style_mixing = self.generator.use_style_mixing,\n",
    "            one_3d_generator = config.one_3d_generator,\n",
    "            dmtet_scale = config.dmtet_scale,\n",
    "            n_implicit_layer = self.generator.n_implicit_layer,\n",
    "            feat_channel = self.generator.feat_channel,\n",
    "            mlp_latent_channel = self.generator.mlp_latent_channel,\n",
    "            deformation_multiplier = self.generator.deformation_multiplier,\n",
    "            tri_plane_resolution = self.generator.tri_plane_resolution,\n",
    "            n_views = self.generator.n_views,\n",
    "            use_tri_plane = self.generator.use_tri_plane,\n",
    "            tet_res = self.generator.tet_res,\n",
    "            latent_dim = self.generator.latent_dim,\n",
    "            geometry_type = self.generator.geometry_type,\n",
    "            render_type = self.generator.render_type\n",
    "        )\n",
    "        loss_and_discriminator= LossAndDiscriminatorConfig(\n",
    "            d_architecture = self.loss_and_discriminator.d_architecture,\n",
    "            use_pl_length = self.loss_and_discriminator.use_pl_length,\n",
    "            gamma_mask =self.loss_and_discriminator.gamma_mask,\n",
    "            d_reg_interval = self.loss_and_discriminator.d_reg_interval,\n",
    "            add_camera_cond = self.loss_and_discriminator.add_camera_cond,\n",
    "            lambda_flexicubes_surface_reg = self.loss_and_discriminator.lambda_flexicubes_surface_reg,\n",
    "            lambda_flexicubes_weights_reg =self.loss_and_discriminator.lambda_flexicubes_weights_reg\n",
    "        )\n",
    "        features= FeaturesConfig(\n",
    "            cond = self.features.cond,\n",
    "            freezed =self.features.freezed\n",
    "        )\n",
    "        hyperparameters= HyperparametersConfig(\n",
    "            batch_gpu = self.hyperparameters.batch_gpu,\n",
    "            cbase =self.hyperparameters.cbase,\n",
    "            cmax =self.hyperparameters.cmax,\n",
    "            glr = self.hyperparameters.glr,\n",
    "            dlr = self.hyperparameters.dlr,\n",
    "            map_depth = self.hyperparameters.map_depth,\n",
    "            mbstd_group =self.hyperparameters.mbstd_group\n",
    "        )\n",
    "        settings= SettingsConfig(\n",
    "            desc= config.desc ,\n",
    "            metrics=self.settings.metrics,\n",
    "            kimg= self.settings.kimg ,\n",
    "            tick=self.settings.tick ,\n",
    "            snap=self.settings.snap ,\n",
    "            seed=self.settings.seed,\n",
    "            fp32=self.settings.fp32 ,\n",
    "            nobench=self.settings.nobench,\n",
    "            workers=self.settings.workers ,\n",
    "            dry_run = self.settings.dry_run,\n",
    "\n",
    "        )\n",
    "\n",
    "        training_model_config = TrainingModelConfig(\n",
    "            styleGAN2 ,\n",
    "            inference,\n",
    "            dataset,\n",
    "            generator,\n",
    "            loss_and_discriminator,\n",
    "            features,\n",
    "            hyperparameters,\n",
    "            settings,\n",
    "        )\n",
    "\n",
    "        return training_model_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textTo3DModelGen import logger\n",
    "from textTo3DModelGen.dnnlib import  EasyDict\n",
    "from textTo3DModelGen.utils.training_utils import *\n",
    "from dataclasses import asdict\n",
    "from textTo3DModelGen.metrics import metric_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingModel:\n",
    "    def __init__(self, config: TrainingModelConfig):\n",
    "        self.config = asdict(config)\n",
    "\n",
    "    def train_step(self):\n",
    "        try:\n",
    "            logger.info(f\"Start Initialize of Config..\")\n",
    "            c = EasyDict()  # Main config dict.\n",
    "            kwargs = {}\n",
    "            for _, value in self.config.items():\n",
    "                kwargs.update(value)\n",
    "            opts = EasyDict(kwargs)  # Command line arguments.\n",
    "            c.G_kwargs = EasyDict(\n",
    "                class_name=None, z_dim=opts.latent_dim,\n",
    "                w_dim=opts.latent_dim,\n",
    "                mapping_kwargs=EasyDict())\n",
    "            c.D_kwargs = EasyDict(\n",
    "                class_name='textTo3DModelGen.training.networks_get3d.Discriminator',\n",
    "                block_kwargs=EasyDict(),\n",
    "                mapping_kwargs=EasyDict(),\n",
    "                epilogue_kwargs=EasyDict())\n",
    "            \n",
    "            c.G_opt_kwargs = EasyDict(class_name='torch.optim.Adam', betas=[0, 0.99], eps=1e-8)\n",
    "            c.D_opt_kwargs = EasyDict(class_name='torch.optim.Adam', betas=[0, 0.99], eps=1e-8)\n",
    "            c.loss_kwargs = EasyDict(class_name='textTo3DModelGen.training.loss.StyleGAN2Loss')\n",
    "\n",
    "            c.data_loader_kwargs = EasyDict(pin_memory=True, prefetch_factor=2)\n",
    "            c.inference_vis = opts.inference_vis\n",
    "\n",
    "            # Training set.\n",
    "            if opts.inference_vis:\n",
    "                c.inference_to_generate_textured_mesh = opts.inference_to_generate_textured_mesh\n",
    "                c.inference_save_interpolation = opts.inference_save_interpolation\n",
    "                c.inference_compute_fid = opts.inference_compute_fid\n",
    "                c.inference_generate_geo = opts.inference_generate_geo\n",
    "\n",
    "            c.training_set_kwargs, dataset_name = init_dataset_kwargs(data=opts.data, opt=opts)\n",
    "            if opts.cond and not c.training_set_kwargs.use_labels:\n",
    "                logger.info(f\"--cond is True but there is not label in dataset.\")\n",
    "                raise ValueError('--cond is True but there is not label in dataset')\n",
    "            \n",
    "            # c.training_set_kwargs.split = 'train' if opts.use_shapenet_split else 'all'\n",
    "            if opts.inference_vis:\n",
    "                c.training_set_kwargs.data_split_file = './artifacts/data_split/test.txt'\n",
    "            c.training_set_kwargs.use_labels = opts.cond\n",
    "            c.training_set_kwargs.xflip = False\n",
    "\n",
    "            # Hyperparameters & settings.p\n",
    "            c.G_kwargs.iso_surface = opts.iso_surface\n",
    "            c.G_kwargs.one_3d_generator = opts.one_3d_generator\n",
    "            c.G_kwargs.n_implicit_layer = opts.n_implicit_layer\n",
    "            c.G_kwargs.deformation_multiplier = opts.deformation_multiplier\n",
    "            c.resume_pretrain = opts.resume_pretrain\n",
    "            c.D_reg_interval = opts.d_reg_interval\n",
    "            c.G_kwargs.use_style_mixing = opts.use_style_mixing\n",
    "            c.G_kwargs.dmtet_scale = opts.dmtet_scale\n",
    "            c.G_kwargs.feat_channel = opts.feat_channel\n",
    "            c.G_kwargs.mlp_latent_channel = opts.mlp_latent_channel\n",
    "            c.G_kwargs.tri_plane_resolution = opts.tri_plane_resolution\n",
    "            c.G_kwargs.n_views = opts.n_views\n",
    "            c.G_kwargs.render_type = opts.render_type\n",
    "            c.G_kwargs.use_tri_plane = opts.use_tri_plane\n",
    "            c.D_kwargs.data_camera_mode = \"mode_obejavers\"\n",
    "            c.D_kwargs.add_camera_cond = opts.add_camera_cond\n",
    "            c.G_kwargs.tet_res = opts.tet_res\n",
    "\n",
    "            c.G_kwargs.geometry_type = opts.geometry_type\n",
    "            c.num_gpus = opts.gpus\n",
    "            c.batch_size = opts.batch\n",
    "            c.batch_gpu = opts.batch_gpu or opts.batch // opts.gpus\n",
    "\n",
    "            # c.G_kwargs.geo_pos_enc = opts.geo_pos_enc\n",
    "            c.G_kwargs.data_camera_mode = 'objaverse'\n",
    "            c.G_kwargs.channel_base = c.D_kwargs.channel_base = opts.cbase\n",
    "            c.G_kwargs.channel_max = c.D_kwargs.channel_max = opts.cmax\n",
    "            c.G_kwargs.mapping_kwargs.num_layers = 8\n",
    "\n",
    "            c.D_kwargs.architecture = opts.d_architecture\n",
    "            c.D_kwargs.block_kwargs.freeze_layers = opts.freezed\n",
    "            c.D_kwargs.epilogue_kwargs.mbstd_group_size = opts.mbstd_group\n",
    "            c.loss_kwargs.gamma_mask = opts.gamma if opts.gamma_mask == 0.0 else opts.gamma_mask\n",
    "            c.loss_kwargs.r1_gamma = opts.gamma\n",
    "            c.loss_kwargs.lambda_flexicubes_surface_reg = opts.lambda_flexicubes_surface_reg\n",
    "            c.loss_kwargs.lambda_flexicubes_weights_reg = opts.lambda_flexicubes_weights_reg\n",
    "            c.G_opt_kwargs.lr = (0.002 if opts.cfg == 'stylegan2' else 0.0025) if opts.glr is None else opts.glr\n",
    "            c.D_opt_kwargs.lr = opts.dlr\n",
    "\n",
    "            c.metrics = opts.metrics\n",
    "            c.total_kimg = opts.kimg\n",
    "            c.kimg_per_tick = opts.tick\n",
    "            c.image_snapshot_ticks = c.network_snapshot_ticks = opts.snap\n",
    "            c.random_seed = c.training_set_kwargs.random_seed = opts.seed\n",
    "            c.data_loader_kwargs.num_workers = opts.workers\n",
    "            c.network_snapshot_ticks = 200\n",
    "\n",
    "            # Sanity checks.\n",
    "            if c.batch_size % c.num_gpus != 0:\n",
    "                raise ValueError('--batch must be a multiple of number of gpus')\n",
    "            if c.batch_size % (c.num_gpus * c.batch_gpu) != 0:\n",
    "                raise ValueError('--batch must be a multiple of number of gpus times --batch-gpu')\n",
    "            if c.batch_gpu < c.D_kwargs.epilogue_kwargs.mbstd_group_size:\n",
    "                raise ValueError('--batch-gpu cannot be smaller than --mbstd')\n",
    "            if any(not metric_main.is_valid_metric(metric) for metric in c.metrics):\n",
    "                raise ValueError(\n",
    "                    '\\n'.join(['--metrics can only contain the following values:'] + metric_main.list_valid_metrics()))\n",
    "            \n",
    "            # Base configuration.\n",
    "            c.ema_kimg = c.batch_size * 10 / 32\n",
    "            c.G_kwargs.class_name = 'textTo3DModelGen.training.networks_get3d.GeneratorDMTETMesh'\n",
    "            c.loss_kwargs.style_mixing_prob = 0.9  # Enable style mixing regularization.\n",
    "            c.loss_kwargs.pl_weight = 0.0  # Enable path length regularization.\n",
    "            c.G_reg_interval = 4  # Enable lazy regularization for G.\n",
    "            c.G_kwargs.fused_modconv_default = 'inference_only'  # Speed up training by using regular convolutions instead of grouped convolutions.\n",
    "            # Performance-related toggles.\n",
    "            if opts.fp32:\n",
    "                c.G_kwargs.num_fp16_res = c.D_kwargs.num_fp16_res = 0\n",
    "                c.G_kwargs.conv_clamp = c.D_kwargs.conv_clamp = None\n",
    "            if opts.nobench:\n",
    "                c.cudnn_benchmark = False\n",
    "\n",
    "            # Description string.\n",
    "            desc = f'{opts.cfg:s}-{dataset_name:s}-gpus{c.num_gpus:d}-batch{c.batch_size:d}-gamma{c.loss_kwargs.r1_gamma:g}'\n",
    "            if opts.desc is not None:\n",
    "                desc += f'-{opts.desc}'\n",
    "\n",
    "            logger.info(f\"Description string {desc}\")\n",
    "            \n",
    "            # Launch.\n",
    "            print('==> launch training')\n",
    "            launch_training(c=c, desc=desc, outdir=opts.outdir, dry_run=opts.dry_run)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-19 00:18:53,513: INFO: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-09-19 00:18:53,549: INFO: yaml file: hyper_params.yaml loaded successfully]\n",
      "[2024-09-19 00:18:53,559: INFO: created directory at: training_logs/]\n",
      "[2024-09-19 00:18:53,564: INFO: Start Initialize of Config..]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> use dataset of folder number 1\n",
      "==> use image path: ./artifacts/dataset\\img, num images: 13\n",
      "[2024-09-19 00:18:53,629: INFO: label shape is [514]]\n",
      "[2024-09-19 00:18:53,632: INFO: Description string stylegan2-objaverse-gpus2-batch16-gamma1000-training_results/]\n",
      "==> launch training\n",
      "[2024-09-19 00:18:53,636: INFO: Training options:]\n",
      "[2024-09-19 00:18:53,638: INFO: {\n",
      "  \"G_kwargs\": {\n",
      "    \"class_name\": \"textTo3DModelGen.training.networks_get3d.GeneratorDMTETMesh\",\n",
      "    \"z_dim\": 512,\n",
      "    \"w_dim\": 512,\n",
      "    \"mapping_kwargs\": {\n",
      "      \"num_layers\": 8\n",
      "    },\n",
      "    \"iso_surface\": \"dmtet\",\n",
      "    \"one_3d_generator\": 0,\n",
      "    \"n_implicit_layer\": 1,\n",
      "    \"deformation_multiplier\": 1.0,\n",
      "    \"use_style_mixing\": 1,\n",
      "    \"dmtet_scale\": 1.0,\n",
      "    \"feat_channel\": 16,\n",
      "    \"mlp_latent_channel\": 32,\n",
      "    \"tri_plane_resolution\": 256,\n",
      "    \"n_views\": 1,\n",
      "    \"render_type\": \"neural_render\",\n",
      "    \"use_tri_plane\": 1,\n",
      "    \"tet_res\": 90,\n",
      "    \"geometry_type\": \"conv3d\",\n",
      "    \"data_camera_mode\": \"objaverse\",\n",
      "    \"channel_base\": 32768,\n",
      "    \"channel_max\": 512,\n",
      "    \"fused_modconv_default\": \"inference_only\"\n",
      "  },\n",
      "  \"D_kwargs\": {\n",
      "    \"class_name\": \"textTo3DModelGen.training.networks_get3d.Discriminator\",\n",
      "    \"block_kwargs\": {\n",
      "      \"freeze_layers\": 0\n",
      "    },\n",
      "    \"mapping_kwargs\": {},\n",
      "    \"epilogue_kwargs\": {\n",
      "      \"mbstd_group_size\": 2\n",
      "    },\n",
      "    \"data_camera_mode\": \"mode_obejavers\",\n",
      "    \"add_camera_cond\": 1,\n",
      "    \"channel_base\": 32768,\n",
      "    \"channel_max\": 512,\n",
      "    \"architecture\": \"skip\"\n",
      "  },\n",
      "  \"G_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.002\n",
      "  },\n",
      "  \"D_opt_kwargs\": {\n",
      "    \"class_name\": \"torch.optim.Adam\",\n",
      "    \"betas\": [\n",
      "      0,\n",
      "      0.99\n",
      "    ],\n",
      "    \"eps\": 1e-08,\n",
      "    \"lr\": 0.002\n",
      "  },\n",
      "  \"loss_kwargs\": {\n",
      "    \"class_name\": \"textTo3DModelGen.training.loss.StyleGAN2Loss\",\n",
      "    \"gamma_mask\": 1000,\n",
      "    \"r1_gamma\": 1000,\n",
      "    \"lambda_flexicubes_surface_reg\": 0.5,\n",
      "    \"lambda_flexicubes_weights_reg\": 0.1,\n",
      "    \"style_mixing_prob\": 0.9,\n",
      "    \"pl_weight\": 0.0\n",
      "  },\n",
      "  \"data_loader_kwargs\": {\n",
      "    \"pin_memory\": true,\n",
      "    \"prefetch_factor\": 2,\n",
      "    \"num_workers\": 3\n",
      "  },\n",
      "  \"inference_vis\": 0,\n",
      "  \"training_set_kwargs\": {\n",
      "    \"class_name\": \"textTo3DModelGen.training.dataset.ImageFolderDataset\",\n",
      "    \"path\": \"./artifacts/dataset\",\n",
      "    \"data_split_file\": \"./artifacts/data_split/train.txt\",\n",
      "    \"use_labels\": 1,\n",
      "    \"max_size\": 13,\n",
      "    \"xflip\": false,\n",
      "    \"resolution\": 128,\n",
      "    \"add_camera_cond\": 1,\n",
      "    \"camera_path\": \"\",\n",
      "    \"random_seed\": 0\n",
      "  },\n",
      "  \"resume_pretrain\": \"\",\n",
      "  \"D_reg_interval\": 16,\n",
      "  \"num_gpus\": 2,\n",
      "  \"batch_size\": 16,\n",
      "  \"batch_gpu\": 4,\n",
      "  \"metrics\": [\n",
      "    \"fid50k\"\n",
      "  ],\n",
      "  \"total_kimg\": 20000,\n",
      "  \"kimg_per_tick\": 1,\n",
      "  \"image_snapshot_ticks\": 10,\n",
      "  \"network_snapshot_ticks\": 200,\n",
      "  \"random_seed\": 0,\n",
      "  \"ema_kimg\": 5.0,\n",
      "  \"G_reg_interval\": 4,\n",
      "  \"run_dir\": \"training_logs/00001-stylegan2-objaverse-gpus2-batch16-gamma1000-training_results/\"\n",
      "}]\n",
      "[2024-09-19 00:18:53,640: INFO: Output directory:    training_logs/00001-stylegan2-objaverse-gpus2-batch16-gamma1000-training_results/]\n",
      "[2024-09-19 00:18:53,643: INFO: Number of GPUs:      2]\n",
      "[2024-09-19 00:18:53,644: INFO: Batch size:          16 images]\n",
      "[2024-09-19 00:18:53,647: INFO: Training duration:   20000 kimg]\n",
      "[2024-09-19 00:18:53,649: INFO: Dataset path:        ./artifacts/dataset]\n",
      "[2024-09-19 00:18:53,652: INFO: Dataset size:        13 images]\n",
      "[2024-09-19 00:18:53,654: INFO: Dataset resolution:  128]\n",
      "[2024-09-19 00:18:53,656: INFO: Dataset labels:      1]\n",
      "[2024-09-19 00:18:53,658: INFO: Dataset x-flips:     False]\n",
      "[2024-09-19 00:18:53,661: INFO: Creating output directory...training_logs/00001-stylegan2-objaverse-gpus2-batch16-gamma1000-training_results/]\n",
      "[2024-09-19 00:18:53,666: INFO: Save the training option to training_logs/00001-stylegan2-objaverse-gpus2-batch16-gamma1000-training_results/training_options.json]\n",
      "[2024-09-19 00:18:53,671: INFO: Launching processes...]\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\training_utils.py\", line 39, in subprocess_fn\n    training_loop_3d.training_loop(rank=rank, **c)\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\training\\training_loop_3d.py\", line 113, in training_loop\n    upfirdn2d._init()\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\torch_utils\\ops\\upfirdn2d.py\", line 33, in _init\n    extra_cuda_cflags=['--use_fast_math'],\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\torch_utils\\custom_ops.py\", line 124, in get_plugin\n    cached_build_dir = os.path.join(build_top_dir, f'{source_digest}-{_get_mangled_gpu_name()}')\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\torch_utils\\custom_ops.py\", line 47, in _get_mangled_gpu_name\n    name = torch.cuda.get_device_name().lower()\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 341, in get_device_name\n    return get_device_properties(device).name\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 371, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 221, in _lazy_init\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\4107268927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtraining_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\4107268927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtraining_model_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_training_model_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtraining_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainingModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtraining_model_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtraining_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\2897229773.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16144\\2897229773.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;31m# Launch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'==> launch training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mlaunch_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdry_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdry_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\training_utils.py\u001b[0m in \u001b[0;36mlaunch_training\u001b[1;34m(c, desc, outdir, dry_run)\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0msubprocess_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtemp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_gpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mparse_comma_separated_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[1;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[0;32m    238\u001b[0m                ' torch.multiprocessing.start_processes(...)' % start_method)\n\u001b[0;32m    239\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'spawn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[1;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\multiprocessing\\spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\training_utils.py\", line 39, in subprocess_fn\n    training_loop_3d.training_loop(rank=rank, **c)\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\training\\training_loop_3d.py\", line 113, in training_loop\n    upfirdn2d._init()\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\torch_utils\\ops\\upfirdn2d.py\", line 33, in _init\n    extra_cuda_cflags=['--use_fast_math'],\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\torch_utils\\custom_ops.py\", line 124, in get_plugin\n    cached_build_dir = os.path.join(build_top_dir, f'{source_digest}-{_get_mangled_gpu_name()}')\n  File \"C:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\src\\textTo3DModelGen\\utils\\torch_utils\\custom_ops.py\", line 47, in _get_mangled_gpu_name\n    name = torch.cuda.get_device_name().lower()\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 341, in get_device_name\n    return get_device_properties(device).name\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 371, in get_device_properties\n    _lazy_init()  # will define _get_device_properties\n  File \"c:\\Users\\lenovo\\Desktop\\Stage\\text-to-3D_Model_Generation\\.venv37\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 221, in _lazy_init\n    raise AssertionError(\"Torch not compiled with CUDA enabled\")\nAssertionError: Torch not compiled with CUDA enabled\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_model_config = config.get_training_model_config()\n",
    "    training_model = TrainingModel(config= training_model_config)\n",
    "    training_model.train_step()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
